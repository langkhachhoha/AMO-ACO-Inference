{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDGLWyK5zY6X"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INOrJdFTzjzv"
      },
      "source": [
        "I - Layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "asy0O4REn8XF"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lbwhPzpVlY4y"
      },
      "outputs": [],
      "source": [
        "device =  torch.device(\"cuda:0\" if True == True else \"cpu\")\n",
        "device = None "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lO_gVRX2zU61"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def scaled_attention(query, key, value, mask=None):\n",
        "    \"\"\" Function that performs scaled attention given q, k, v and mask.\n",
        "    q, k, v can have multiple batches and heads, defined across the first dimensions\n",
        "    and the last 2 dimensions for a given sample of them are in row vector format.\n",
        "    matmul is brodcasted across batches.\n",
        "    \"\"\"\n",
        "    qk = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n",
        "    if mask is not None: qk = qk.masked_fill(mask == 1, -1e9)\n",
        "    qk = F.softmax(qk, dim=-1)\n",
        "    return torch.matmul(qk, value)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Attention Layer - multi-head scaled dot product attention (for encoder and decoder)\n",
        "        Observation: This MHA is currently implemented to only support singe-gpu machines\n",
        "\n",
        "        Args:\n",
        "            num_heads: number of attention heads which will be computed in parallel\n",
        "            d_model: embedding size of output AND input features\n",
        "            * in reality it shouldn't be neccesary that input and ouptut features are the same dimension\n",
        "              but its the current case for this class.\n",
        "\n",
        "        Call arguments:\n",
        "            q: query, shape (..., seq_len_q, depth_q)\n",
        "            k: key, shape == (..., seq_len_k, depth_k)\n",
        "            v: value, shape == (..., seq_len_v, depth_v)\n",
        "            mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k) or None.\n",
        "\n",
        "            Since we use scaled-product attention, we assume seq_len_k = seq_len_v\n",
        "\n",
        "        Returns:\n",
        "              attention outputs of shape (batch_size, seq_len_q, d_model)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_heads, d_model, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads, self.d_model = n_heads, d_model\n",
        "        self.head_depth = self.d_model // self.n_heads\n",
        "\n",
        "        assert self.d_model % self.n_heads == 0\n",
        "\n",
        "        # define weight matrices\n",
        "        self.wq = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wk = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.wv = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "        self.w_out = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "    def split_heads(self, tensor, batch_size):\n",
        "        \"\"\" Function that splits the heads. This happens in the same tensor since this class doesn't\n",
        "        support multiple-gpu. Observe inline comments for more details on shapes.\n",
        "        \"\"\"\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, n_heads, head_depth)\n",
        "        splitted_tensor = tensor.view(batch_size, -1, self.n_heads, self.head_depth)\n",
        "        return splitted_tensor.transpose(1, 2) # (batch_size, n_heads, seq_len, head_depth)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # shape of q: (batch_size, seq_len_q, d_query)\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # project query, key and value to d_model dimensional space\n",
        "        # this is equivalent to projecting them each to a head_depth dimensional space (for every head)\n",
        "        # but with a single matrix\n",
        "        Q = self.wq(query) # (batch_size, seq_len_q, d_query) -> (batch_size, seq_len_q, d_model)\n",
        "        K = self.wk(key) # ... -> (batch_size, seq_len_k, d_model)\n",
        "        V = self.wv(value) # ... -> (batch_size, seq_len_v, d_model)\n",
        "\n",
        "        # split individual heads\n",
        "        Q = self.split_heads(Q, batch_size) # ... -> (batch_size, n_heads, seq_len_q, head_depth)\n",
        "        K = self.split_heads(K, batch_size) # ... -> (batch_size, n_heads, seq_len_k, head_depth)\n",
        "        V = self.split_heads(V, batch_size) # ... -> (batch_size, n_heads, seq_len_v, head_depth)\n",
        "\n",
        "\n",
        "        # Add dimension to mask so that it can be broadcasted across heads\n",
        "        # (batch_size, seq_len_q, seq_len_k) --> (batch_size, 1, seq_len_q, seq_len_k)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        # perform attention for each q=(seq_len_q, head_depth), k=(seq_len_k, head_depth), v=(seq_len_v, head_depth)\n",
        "        attention = scaled_attention(Q, K, V, mask) # (batch_size, n_heads, seq_len_q, head_depth)\n",
        "        # transpose attention to (batch_size, seq_len_q, n_heads, head_depth)\n",
        "        attention = attention.transpose(1, 2).contiguous()\n",
        "        # concatenate results of all heads (batch_size, seq_len_q, self.d_model)\n",
        "        attention = attention.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # project attention to same dimension; observe this is equivalent to summing individual projection\n",
        "        # as sugested in paper\n",
        "        output = self.w_out(attention) # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKs0YtrFz0JB"
      },
      "source": [
        "II - Graph-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JPplzxDhzwS7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    \"\"\"Feed-Forward Sublayer: fully-connected Feed-Forward network,\n",
        "    built based on MHA vectors from MultiHeadAttention layer with skip-connections\n",
        "\n",
        "        Args:\n",
        "            num_heads: number of attention heads in MHA layers.\n",
        "            input_dim: embedding size that will be used as d_model in MHA layers.\n",
        "            feed_forward_hidden: number of neuron units in each FF layer.\n",
        "\n",
        "        Call arguments:\n",
        "            x: batch of shape (batch_size, n_nodes, node_embedding_size).\n",
        "            mask: mask for MHA layer\n",
        "\n",
        "        Returns:\n",
        "               outputs of shape (batch_size, n_nodes, input_dim)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, feed_forward_hidden=512, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mha = MultiHeadAttention(n_heads=num_heads, d_model=input_dim)\n",
        "\n",
        "        self.ff1 = nn.Linear(input_dim, feed_forward_hidden)\n",
        "        self.ff2 = nn.Linear(feed_forward_hidden, input_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        mha_out = self.mha(x, x, x, mask)\n",
        "        sc1_out = torch.add(x, mha_out)\n",
        "        tanh1_out = torch.tanh(sc1_out)\n",
        "\n",
        "        ff1_out = self.ff1(tanh1_out)\n",
        "        relu1_out = F.relu(ff1_out)\n",
        "        ff2_out = self.ff2(relu1_out)\n",
        "        sc2_out = torch.add(tanh1_out, ff2_out)\n",
        "        tanh2_out = torch.tanh(sc2_out)\n",
        "\n",
        "        return tanh2_out\n",
        "\n",
        "class GraphAttentionEncoder(nn.Module):\n",
        "    \"\"\"Graph Encoder, which uses MultiHeadAttentionLayer sublayer.\n",
        "\n",
        "        Args:\n",
        "            input_dim: embedding size that will be used as d_model in MHA layers.\n",
        "            num_heads: number of attention heads in MHA layers.\n",
        "            num_layers: number of attention layers that will be used in encoder.\n",
        "            feed_forward_hidden: number of neuron units in each FF layer.\n",
        "\n",
        "        Call arguments:\n",
        "            x: tuples of 3 tensors:  (batch_size, 2), (batch_size, n_nodes-1, 2), (batch_size, n_nodes-1)\n",
        "            First tensor contains coordinates for depot, second one is for coordinates of other nodes,\n",
        "            Last tensor is for normalized demands for nodes except depot\n",
        "\n",
        "            mask: mask for MHA layer\n",
        "\n",
        "        Returns:\n",
        "               Embedding for all nodes + mean embedding for graph.\n",
        "               Tuples ((batch_size, n_nodes, input_dim), (batch_size, input_dim))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, num_layers, feed_forward_hidden=512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.feed_forward_hidden = feed_forward_hidden\n",
        "\n",
        "        # initial embeddings (batch_size, n_nodes-1, 2) --> (batch-size, input_dim), separate for depot and other nodes\n",
        "        self.init_embed_depot = nn.Linear(2, self.input_dim)  # nn.Linear(2, embedding_dim)\n",
        "        self.init_embed = nn.Linear(6, self.input_dim) # x,y,demand,time,finish,service\n",
        "\n",
        "        self.mha_layers = [MultiHeadAttentionLayer(self.input_dim, self.num_heads, self.feed_forward_hidden)\n",
        "                            for _ in range(self.num_layers)]\n",
        "        self.mha_layers = nn.ModuleList(self.mha_layers)\n",
        "\n",
        "    def forward(self, x, mask=None, cur_num_nodes=None):\n",
        "        # print(x)\n",
        "\n",
        "        x = torch.cat((self.init_embed_depot(x[0])[:, None, :],  # (batch_size, 2) --> (batch_size, 1, 2)\n",
        "                       self.init_embed(torch.cat((x[1], x[2][:, :, None], x[3][:, :, None], x[4][:, :, None], x[5][:, :, None]), -1))  # (batch_size, n_nodes-1, 2) + (batch_size, n_nodes-1)\n",
        "                       ), 1)         # (batch_size, n_nodes, input_dim)\n",
        "\n",
        "\n",
        "        # stack attention layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.mha_layers[i](x, mask)\n",
        "\n",
        "        if mask is not None:\n",
        "            output = (x, torch.sum(x, 1) / cur_num_nodes)\n",
        "        else:\n",
        "            output = (x, torch.mean(x, 1))\n",
        "\n",
        "        return output # (embeds of nodes, avg graph embed)=((batch_size, n_nodes, input), (batch_size, input_dim))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ADRRL0K0O3J"
      },
      "source": [
        "III - Attention Dynamic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VVj4NOYh0ODG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.categorical import Categorical\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def get_dev_of_mod(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "def set_decode_type(model, decode_type):\n",
        "    model.set_decode_type(decode_type)\n",
        "\n",
        "class AttentionDynamicModel(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_dim,\n",
        "                 n_encode_layers=2,\n",
        "                 n_heads=8,\n",
        "                 tanh_clipping=10.,\n",
        "                 device = device\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # attributes for MHA\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_encode_layers = n_encode_layers\n",
        "        self.decode_type = None\n",
        "\n",
        "        # attributes for VRP problem\n",
        "        self.problem = AgentVRP\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Encoder part\n",
        "        self.embedder = GraphAttentionEncoder(input_dim=self.embedding_dim,\n",
        "                                              num_heads=self.n_heads,\n",
        "                                              num_layers=self.n_encode_layers\n",
        "                                              )\n",
        "\n",
        "        # Decoder part\n",
        "\n",
        "        self.output_dim = self.embedding_dim\n",
        "        self.num_heads = n_heads\n",
        "\n",
        "        self.head_depth = self.output_dim // self.num_heads\n",
        "        self.dk_mha_decoder = float(self.head_depth)  # for decoding in mha_decoder\n",
        "        self.dk_get_loc_p = float(self.output_dim)  # for decoding in mha_decoder\n",
        "\n",
        "        if self.output_dim % self.num_heads != 0:\n",
        "            raise ValueError(\"number of heads must divide d_model=output_dim\")\n",
        "\n",
        "        self.tanh_clipping = tanh_clipping\n",
        "\n",
        "        # we split projection matrix Wq into 2 matrices: Wq*[h_c, h_N, D] = Wq_context*h_c + Wq_step_context[h_N, D]\n",
        "        self.wq_context = nn.Linear(self.embedding_dim, self.output_dim)  # (d_q_context, output_dim)\n",
        "        self.wq_step_context = nn.Linear(self.embedding_dim + 1, self.output_dim, bias=False)  # (d_q_step_context, output_dim)\n",
        "\n",
        "        # we need two Wk projections since there is MHA followed by 1-head attention - they have different keys K\n",
        "        self.wk = nn.Linear(self.embedding_dim, self.output_dim, bias=False)  # (d_k, output_dim)\n",
        "        self.wk_tanh = nn.Linear(self.embedding_dim, self.output_dim, bias=False)  # (d_k_tanh, output_dim)\n",
        "\n",
        "        # we dont need Wv projection for 1-head attention: only need attention weights as outputs\n",
        "        self.wv = nn.Linear(self.embedding_dim, self.output_dim, bias=False)  # (d_v, output_dim)\n",
        "\n",
        "        # we dont need wq for 1-head tanh attention, since we can absorb it into w_out\n",
        "        self.w_out = nn.Linear(self.embedding_dim, self.output_dim, bias=False)  # (d_model, d_model)\n",
        "\n",
        "        self.dev = device\n",
        "\n",
        "    def set_decode_type(self, decode_type):\n",
        "        self.decode_type = decode_type\n",
        "\n",
        "    def split_heads(self, tensor, batch_size):\n",
        "        \"\"\"Function for computing attention on several heads simultaneously\n",
        "        Splits tensor to be multi headed.\n",
        "        \"\"\"\n",
        "        # (batch_size, seq_len, output_dim) -> (batch_size, seq_len, n_heads, head_depth)\n",
        "        splitted_tensor = tensor.view(batch_size, -1, self.n_heads, self.head_depth)\n",
        "        return splitted_tensor.transpose(1, 2) # (batch_size, n_heads, seq_len, head_depth)\n",
        "\n",
        "    def _select_node(self, logits):\n",
        "        \"\"\"Select next node based on decoding type.\n",
        "        \"\"\"\n",
        "\n",
        "        # assert tf.reduce_all(logits) == logits, \"Probs should not contain any nans\"\n",
        "\n",
        "        if self.decode_type == \"greedy\":\n",
        "            selected = torch.argmax(logits, dim=-1)  # (batch_size, 1)\n",
        "\n",
        "        elif self.decode_type == \"sampling\":\n",
        "            # logits has a shape of (batch_size, 1, n_nodes), we have to squeeze it\n",
        "            # to (batch_size, n_nodes) since tf.random.categorical requires matrix\n",
        "            cat_dist = Categorical(logits=logits[:, 0, :]) # creates categorical distribution from tensor (batch_size)\n",
        "            selected = cat_dist.sample() # takes a single sample from distribution\n",
        "        else:\n",
        "            assert False, \"Unknown decode type\"\n",
        "\n",
        "        return torch.squeeze(selected, -1)  # (batch_size,)\n",
        "\n",
        "    def get_step_context(self, state, embeddings):\n",
        "        \"\"\"Takes a state and graph embeddings,\n",
        "           Returns a part [h_N, D] of context vector [h_c, h_N, D],\n",
        "           that is related to RL Agent last step.\n",
        "        \"\"\"\n",
        "        # index of previous node\n",
        "        prev_node = state.prev_a.to(self.dev)  # (batch_size, 1)\n",
        "\n",
        "        # from embeddings=(batch_size, n_nodes, input_dim) select embeddings of previous nodes\n",
        "        cur_embedded_node = embeddings.gather(1, prev_node.view(prev_node.shape[0], -1, 1)\n",
        "                            .repeat_interleave(embeddings.shape[-1], -1)) # (batch_size, 1, input_dim)\n",
        "\n",
        "        # add remaining capacity\n",
        "        step_context = torch.cat([cur_embedded_node, (self.problem.VEHICLE_CAPACITY - state.used_capacity[:, :, None]).to(self.dev)], dim=-1)\n",
        "\n",
        "        return step_context  # (batch_size, 1, input_dim + 1)\n",
        "\n",
        "    def decoder_mha(self, Q, K, V, mask=None):\n",
        "        \"\"\" Computes Multi-Head Attention part of decoder\n",
        "        Args:\n",
        "            mask: a mask for visited nodes,\n",
        "                has shape (batch_size, seq_len_q, seq_len_k), seq_len_q = 1 for context vector attention in decoder\n",
        "            Q: query (context vector for decoder)\n",
        "                    has shape (batch_size, n_heads, seq_len_q, head_depth) with seq_len_q = 1 for context_vector attention in decoder\n",
        "            K, V: key, value (projections of nodes embeddings)\n",
        "                have shape (batch_size, n_heads, seq_len_k, head_depth), (batch_size, n_heads, seq_len_v, head_depth),\n",
        "                                                                with seq_len_k = seq_len_v = n_nodes for decoder\n",
        "        \"\"\"\n",
        "\n",
        "        # Add dimension to mask so that it can be broadcasted across heads\n",
        "        # (batch_size, seq_len_q, seq_len_k) --> (batch_size, 1, seq_len_q, seq_len_k)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        attention = scaled_attention(Q, K, V, mask) # (batch_size, n_heads, seq_len_q, head_depth)\n",
        "        # transpose attention to (batch_size, seq_len_q, n_heads, head_depth)\n",
        "        attention = attention.transpose(1, 2).contiguous()\n",
        "        # concatenate results of all heads (batch_size, seq_len_q, self.output_dim)\n",
        "        attention = attention.view(self.batch_size, -1, self.output_dim)\n",
        "\n",
        "        output = self.w_out(attention)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_log_p(self, Q, K, mask=None):\n",
        "        \"\"\"Single-Head attention sublayer in decoder,\n",
        "        computes log-probabilities for node selection.\n",
        "\n",
        "        Args:\n",
        "            mask: mask for nodes\n",
        "            Q: query (output of mha layer)\n",
        "                    has shape (batch_size, seq_len_q, output_dim), seq_len_q = 1 for context attention in decoder\n",
        "            K: key (projection of node embeddings)\n",
        "                    has shape  (batch_size, seq_len_k, output_dim), seq_len_k = n_nodes for decoder\n",
        "        \"\"\"\n",
        "\n",
        "        compatibility = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.shape[-1])\n",
        "        compatibility = torch.tanh(compatibility) * self.tanh_clipping\n",
        "        if mask is not None: compatibility = compatibility.masked_fill(mask == 1, -1e9)\n",
        "\n",
        "        log_p = F.log_softmax(compatibility, dim=-1)  # (batch_size, seq_len_q, seq_len_k)\n",
        "\n",
        "        return log_p\n",
        "\n",
        "    def get_likelihood_selection(self, _log_p, a):\n",
        "\n",
        "        # Get log_p corresponding to selected actions for every batch\n",
        "        indices = a.view(a.shape[0], -1)\n",
        "        select = _log_p.gather(-1, indices)\n",
        "        return select.view(-1)\n",
        "\n",
        "\n",
        "    def get_projections(self, embeddings, context_vectors):\n",
        "\n",
        "        # we compute some projections (common for each policy step) before decoding loop for efficiency\n",
        "        K = self.wk(embeddings)  # (batch_size, n_nodes, output_dim)\n",
        "        K_tanh = self.wk_tanh(embeddings)  # (batch_size, n_nodes, output_dim)\n",
        "        V = self.wv(embeddings)  # (batch_size, n_nodes, output_dim)\n",
        "        Q_context = self.wq_context(context_vectors[:, None, :])  # (batch_size, 1, output_dim)\n",
        "\n",
        "        # we dont need to split K_tanh since there is only 1 head; Q will be split in decoding loop\n",
        "        K = self.split_heads(K, self.batch_size)  # (batch_size, num_heads, n_nodes, head_depth)\n",
        "        V = self.split_heads(V, self.batch_size)  # (batch_size, num_heads, n_nodes, head_depth)\n",
        "\n",
        "        return K_tanh, Q_context, K, V\n",
        "\n",
        "\n",
        "    def fwd_rein_loss(self, inputs, baseline, bl_vals, num_batch, return_pi=False):\n",
        "        \"\"\"\n",
        "        Forward and calculate loss for REINFORCE algorithm in a memory efficient way.\n",
        "        This sacrifices a bit of performance but is way better in memory terms and works\n",
        "        by reordering the terms in the gradient formula such that we don't store gradients\n",
        "        for all the seguence for a long time which hence produces a lot of memory consumption.\n",
        "        \"\"\"\n",
        "\n",
        "        on_training = self.training\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            cost, log_likelihood, seq = self(inputs, True)\n",
        "            bl_val = bl_vals[num_batch] if bl_vals is not None else baseline.eval(inputs, cost)\n",
        "            pre_cost = cost - bl_val.detach()\n",
        "            detached_loss = torch.mean((pre_cost) * log_likelihood)\n",
        "\n",
        "        if on_training: self.train()\n",
        "        return detached_loss, self(inputs, return_pi, seq, pre_cost)\n",
        "\n",
        "    def forward(self, inputs, return_pi=False, pre_selects=None, pre_cost=None):\n",
        "        \"\"\"\n",
        "        Forward method. Works as expected except and as described on the paper, however\n",
        "        if pre_selects is None which hence implies that pre_cost should be none it's because\n",
        "        fwd_rein_loss is calling it; check that method for a description of why this is useful.\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size = inputs[0].shape[0]\n",
        "        # print(inputs)\n",
        "        # inputs = self.set_input_device(inputs) # sent inputs to GPU for training if it's being used\n",
        "\n",
        "        state = self.problem(inputs) # use CPU inputs for state\n",
        "        inputs = self.set_input_device(inputs) # sent inputs to GPU for training if it's being used\n",
        "        # print(inputs)\n",
        "\n",
        "        sequences = []\n",
        "        ll = torch.zeros(self.batch_size)\n",
        "\n",
        "        if pre_selects is not None:\n",
        "            pre_selects = pre_selects.transpose(0, 1)\n",
        "        # Perform decoding steps\n",
        "        pre_select_idx = 0\n",
        "        state.i = torch.zeros(1, dtype=torch.int64)\n",
        "        att_mask, cur_num_nodes = state.get_att_mask()\n",
        "        att_mask, cur_num_nodes = att_mask.to(self.dev), cur_num_nodes.to(self.dev)\n",
        "        embeddings, context_vectors = self.embedder(inputs, att_mask, cur_num_nodes)\n",
        "        K_tanh, Q_context, K, V = self.get_projections(embeddings, context_vectors)\n",
        "        while not state.all_finished():\n",
        "\n",
        "            # state.i = torch.zeros(1, dtype=torch.int64)\n",
        "            # att_mask, cur_num_nodes = state.get_att_mask()\n",
        "            # att_mask, cur_num_nodes = att_mask.to(self.dev), cur_num_nodes.to(self.dev)\n",
        "            # embeddings, context_vectors = self.embedder(inputs, att_mask, cur_num_nodes)\n",
        "            # K_tanh, Q_context, K, V = self.get_projections(embeddings, context_vectors)\n",
        "\n",
        "            # while not state.partial_finished():\n",
        "\n",
        "                step_context = self.get_step_context(state, embeddings)  # (batch_size, 1, input_dim + 1)\n",
        "                Q_step_context = self.wq_step_context(step_context)  # (batch_size, 1, output_dim)\n",
        "                Q = Q_context + Q_step_context\n",
        "\n",
        "                # split heads for Q\n",
        "                Q = self.split_heads(Q, self.batch_size)  # (batch_size, num_heads, 1, head_depth)\n",
        "\n",
        "                # get current mask\n",
        "                mask = state.get_mask().to(self.dev)  # (batch_size, 1, n_nodes) True -> mask, i.e. agent can NOT go\n",
        "\n",
        "                # compute MHA decoder vectors for current mask\n",
        "                mha = self.decoder_mha(Q, K, V, mask)  # (batch_size, 1, output_dim)\n",
        "\n",
        "                # compute probabilities\n",
        "                log_p = self.get_log_p(mha, K_tanh, mask)  # (batch_size, 1, n_nodes)\n",
        "\n",
        "                # next step is to select node\n",
        "                if pre_selects is None:\n",
        "                    selected = self._select_node(log_p.detach()) # (batch_size,)\n",
        "                else:\n",
        "                    selected = pre_selects[pre_select_idx]\n",
        "\n",
        "                state.step(selected.detach().cpu())\n",
        "\n",
        "                curr_ll = self.get_likelihood_selection(log_p[:, 0, :].cpu(), selected.detach().cpu())\n",
        "                if pre_selects is not None:\n",
        "                    curr_loss = (curr_ll * pre_cost).sum() / self.batch_size\n",
        "                    curr_loss.backward(retain_graph=True)\n",
        "                    curr_ll = curr_ll.detach()\n",
        "                ll += curr_ll\n",
        "\n",
        "                sequences.append(selected.detach().cpu())\n",
        "                pre_select_idx += 1\n",
        "                # torch.cuda.empty_cache()\n",
        "            # torch.cuda.empty_cache()\n",
        "\n",
        "        pi = torch.stack(sequences, dim=1) # (batch_size, len(outputs))\n",
        "        cost = self.problem.get_costs((inputs[0].detach().cpu(), inputs[1].detach().cpu(), inputs[2].detach().cpu()), pi)\n",
        "\n",
        "        ret = [cost, ll]\n",
        "        if return_pi: ret.append(pi)\n",
        "        return ret\n",
        "\n",
        "    def set_input_device(self, inp_tens):\n",
        "        if self.dev is None: self.dev = get_dev_of_mod(self)\n",
        "        return(inp_tens[0].to(device), inp_tens[1].to(device), inp_tens[2].to(device), inp_tens[3].to(device), inp_tens[4].to(device), inp_tens[5].to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzWLjgfI1N1W"
      },
      "source": [
        "Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QzMTvcCH1Pen"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class AgentVRP():\n",
        "    VEHICLE_CAPACITY = 1.0\n",
        "\n",
        "    def __init__(self, input):\n",
        "        depot = input[0] # (batch_size, 2)\n",
        "        loc = input[1] # (batch_size, n_nodes, 2)\n",
        "        self.demand = input[2] # (batch_size, n_nodes)\n",
        "        self.time = input[3]\n",
        "        self.finish = input[4]\n",
        "        self.service = input[5]\n",
        "\n",
        "        self.batch_size, self.n_loc, _ = loc.shape\n",
        "\n",
        "        # Coordinates of depot + other nodes -> (batch_size, 1+n_nodes, 2)\n",
        "        self.coords = torch.cat((depot[:, None, :], loc), dim=-2)\n",
        "\n",
        "        # Indices of graphs in batch\n",
        "        self.ids = torch.arange(self.batch_size) # (batch_size)\n",
        "\n",
        "        # State\n",
        "        self.prev_a = torch.zeros(self.batch_size, 1, dtype=torch.int64)\n",
        "        self.from_depot = (self.prev_a == 0)\n",
        "        self.used_capacity = torch.zeros(self.batch_size, 1)\n",
        "        self.used_time = torch.zeros(self.batch_size, 1)\n",
        "        self.ids=torch.arange(self.batch_size, dtype=torch.int64)[:, None]\n",
        "        self.ids_1=torch.arange(self.batch_size, dtype=torch.int64)\n",
        "\n",
        "        # Nodes that have been visited will be marked with 1\n",
        "        self.visited = torch.zeros(self.batch_size, 1, self.n_loc+1)\n",
        "\n",
        "        # Step counter\n",
        "        self.i = torch.zeros(1, dtype=torch.int64)\n",
        "\n",
        "    @staticmethod\n",
        "    def outer_pr(a, b):\n",
        "        \"\"\" Outer product of a and b row vectors.\n",
        "            result[k] = matmul( a[k].t(), b[k] )\n",
        "        \"\"\"\n",
        "        return torch.einsum('ki,kj->kij', a, b)\n",
        "\n",
        "    def get_att_mask(self):\n",
        "        \"\"\" Mask (batchsize, n_nodes, n_nodes) for attention encoder.\n",
        "            We maks alredy visited nodes except for depot (can be visited multiple times).\n",
        "\n",
        "            True -> should mask (can NOT visit)\n",
        "            False -> shouldn't mask (can visit)\n",
        "        \"\"\"\n",
        "        # Remove depot from mask (1st column)\n",
        "        att_mask = torch.squeeze(self.visited, dim=-2)[:, 1:] # (batch_size, 1, n_nodes) -> (batch_size, n_nodes-1)\n",
        "\n",
        "        # Number of nodes in new instance after masking\n",
        "        cur_num_nodes = self.n_loc + 1 - att_mask.sum(dim=1, keepdims=True) # (batch_size, 1)\n",
        "\n",
        "        att_mask = torch.cat((torch.zeros(att_mask.shape[0], 1), att_mask), dim=-1) # add depot -> (batch_size, n_nodes)\n",
        "\n",
        "        ones_mask = torch.ones_like(att_mask)\n",
        "\n",
        "        # Create square attention mask.\n",
        "        # In a (n_nodes, n_nodes) matrix this masks all rows and columns of visited nodes\n",
        "        att_mask = AgentVRP.outer_pr(att_mask, ones_mask) \\\n",
        "                    + AgentVRP.outer_pr(ones_mask, att_mask) \\\n",
        "                    - AgentVRP.outer_pr(att_mask, att_mask) # (batch_size, n_nodes, n_nodes)\n",
        "        return att_mask == 1, cur_num_nodes\n",
        "\n",
        "    def all_finished(self):\n",
        "        \"\"\" Checks if all routes are finished\n",
        "        \"\"\"\n",
        "        return torch.all(self.visited == 1).item()\n",
        "\n",
        "    def partial_finished(self):\n",
        "        \"\"\"Checks if partial solution for all graphs has been built; i.e. all agents came back to depot\n",
        "        \"\"\"\n",
        "        return (torch.all(self.from_depot == 1) and self.i != 0).item()\n",
        "    def dist(self): #ok\n",
        "        return (self.coords[:, :, None, :] - self.coords[:, None, :, :]).norm(p=2, dim=-1)\n",
        "\n",
        "    def get_mask(self):\n",
        "        \"\"\" Returns a mask (batch_size, 1, n_nodes) with available actions.\n",
        "            Impossible nodes are masked.\n",
        "\n",
        "            True -> should mask (can NOT visit)\n",
        "            False -> shouldn't mask (can visit)\n",
        "        \"\"\"\n",
        "\n",
        "        # Exclude depot\n",
        "        visited_loc = self.visited[:, :, 1:]\n",
        "\n",
        "        # Mark nodes which exceed vehicle capacity\n",
        "        exceeds_cap = self.demand + self.used_capacity > self.VEHICLE_CAPACITY\n",
        "\n",
        "        # For demand steps_dim is inserted by indexing with id, for used_capacity insert node dim for broadcasting\n",
        "\n",
        "\n",
        "        exceeds_time = ((self.service[self.ids, torch.clamp(self.prev_a - 1, 0, self.n_loc - 1)] * (self.prev_a != 0)).view(self.batch_size,1,1).float() + self.dist()[self.ids, self.prev_a, 1:] > self.finish[:,None,:])\n",
        "\n",
        "\n",
        "\n",
        "        mask_loc = (visited_loc == 1) | (exceeds_cap[:, None, :]) | exceeds_time\n",
        "        # print('mask_loc', mask_loc)\n",
        "\n",
        "\n",
        "        # We can choose depot if we are not in depot OR all nodes are visited\n",
        "        # equivalent to: we mask the depot if we are in it AND there're still mode nodes to visit\n",
        "        mask_depot = self.from_depot[:, None, :] & ((mask_loc == False).sum(dim=-1, keepdims=True) > 0)\n",
        "\n",
        "        return torch.cat([mask_depot, mask_loc], dim=-1)\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        # Update current state\n",
        "        selected = action[:, None]\n",
        "\n",
        "        cur_coord = self.coords[self.ids, selected]\n",
        "        selected_time = self.service[self.ids, torch.clamp(self.prev_a - 1, 0, self.n_loc - 1)] + (cur_coord - self.coords[self.ids, self.prev_a]).norm(p=2, dim=-1)\n",
        "\n",
        "        used_time = self.used_time + selected_time\n",
        "        time = self.time[self.ids, torch.clamp(selected - 1, 0, self.n_loc - 1)]\n",
        "        # used_time = time\n",
        "\n",
        "        used_time = torch.where(used_time < time, time, used_time)\n",
        "        used_time_1 = (used_time * (selected != 0)).float()\n",
        "        self.used_time = used_time_1\n",
        "        # print('Used_time', self.used_time)\n",
        "        self.prev_a = selected\n",
        "        self.from_depot = self.prev_a == 0\n",
        "\n",
        "        # Shift indices by 1 since self.demand doesn't consider depot\n",
        "        selected_demand = self.demand.gather(-1, (self.prev_a - 1).clamp_min(0).view(-1, 1)) # (batch_size, 1)\n",
        "\n",
        "        # Add current node capacity to used capacity and set it to 0 if we return from depot\n",
        "        self.used_capacity = (self.used_capacity + selected_demand) * (self.from_depot == False)\n",
        "\n",
        "\n",
        "        # Update visited nodes (set 1 to visited nodes)\n",
        "        self.prev_a = selected\n",
        "        self.from_depot = self.prev_a == 0\n",
        "        # print('Before', self.visited)\n",
        "        # print('action', action)\n",
        "        # print('selected', selected)\n",
        "\n",
        "        self.visited[self.ids_1, [0], action] = 1\n",
        "        # print('After', self.visited)\n",
        "\n",
        "        self.i += 1\n",
        "\n",
        "    @staticmethod\n",
        "    def get_costs(dataset, pi):\n",
        "\n",
        "        # Place nodes with coordinates in order of decoder tour\n",
        "        loc_with_depot = torch.cat((dataset[0][:, None, :], dataset[1]), dim=1) # (batch_size, n_nodes, 2)\n",
        "        d = loc_with_depot.gather(1, pi.view(pi.shape[0], -1, 1).repeat_interleave(2, -1))\n",
        "\n",
        "        # Calculation of total distance\n",
        "        # Note: first element of pi is not depot, but the first selected node in path\n",
        "        # and last element from longest path is not depot\n",
        "\n",
        "        return ((torch.norm(d[:, 1:] - d[:, :-1], dim=-1)).sum(dim=-1) # intra node distances\n",
        "            + (torch.norm(d[:, 0] - dataset[0], dim=-1))  # distance from depot to first\n",
        "            + (torch.norm(d[:, -1] - dataset[0], dim=-1))) # distance from last node of longest path to depot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u2KED4T1t0e"
      },
      "source": [
        "Ultis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8uMv8mNy1sut"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "class Data_100():\n",
        "    def __init__(self):\n",
        "        self.size = 200\n",
        "        self.graph_size = 100\n",
        "        self.time_factor = 100\n",
        "        self.service_duration = 10\n",
        "        self.tw_expansion = 3\n",
        "        self.service_window = 1000\n",
        "        self.loc = 15\n",
        "        self.scale = 10\n",
        "        self.max = 42\n",
        "        self.q = 1\n",
        "\n",
        "def generate_data_onfly(cfg, rnds=None,\n",
        "                         ):\n",
        "\n",
        "    rnds = np.random if rnds is None else rnds\n",
        "\n",
        "    # sample locations\n",
        "    dloc = rnds.uniform(size=(cfg.size, 2))  # depot location\n",
        "    nloc = rnds.uniform(size=(cfg.size, cfg.graph_size, 2))  # node locations\n",
        "\n",
        "    # TW start needs to be feasibly reachable directly from depot\n",
        "    min_t = np.ceil(np.linalg.norm(dloc[:, None, :]*cfg.time_factor - nloc*cfg.time_factor, axis=-1)) + 1\n",
        "    # TW end needs to be early enough to perform service and return to depot until end of service window\n",
        "    max_t = np.ceil(np.linalg.norm(dloc[:, None, :]*cfg.time_factor - nloc*cfg.time_factor, axis=-1) + cfg.service_duration) + 1\n",
        "\n",
        "    # horizon allows for the feasibility of reaching nodes / returning from nodes within the global tw (service window)\n",
        "    horizon = list(zip(min_t, cfg.service_window - max_t))\n",
        "    epsilon = np.maximum(np.abs(rnds.standard_normal([cfg.size, cfg.graph_size])), 1 / cfg.time_factor)\n",
        "\n",
        "    # sample earliest start times a\n",
        "    a = [rnds.randint(*h) for h in horizon]\n",
        "\n",
        "    tw = [np.transpose(np.vstack((rt,  # a\n",
        "                                  np.minimum(rt + cfg.tw_expansion * cfg.time_factor * sd, h[-1]).astype(int)  # b\n",
        "                                  ))).tolist()\n",
        "          for rt, sd, h in zip(a, epsilon, horizon)]\n",
        "\n",
        "    depo = torch.FloatTensor(dloc.tolist()) * cfg.time_factor / cfg.service_window\n",
        "    graphs = torch.FloatTensor(nloc.tolist()) * cfg.time_factor / cfg.service_window\n",
        "    demand = torch.FloatTensor(np.minimum(np.maximum(np.abs(rnds.normal(loc=cfg.loc, scale=cfg.scale, size=[cfg.size, cfg.graph_size])).astype(int), 1), cfg.max).tolist()) / 200.\n",
        "    tw = torch.FloatTensor(tw) / cfg.service_window\n",
        "    time = tw[:,:,0]\n",
        "    finish = tw[:,:,1]\n",
        "    service = torch.tensor(np.full([cfg.size, cfg.graph_size], cfg.service_duration).tolist()) / cfg.service_window\n",
        "\n",
        "    return (depo, graphs, demand, time, finish, service)\n",
        "              \n",
        "\n",
        "\n",
        "\n",
        "# def generate_data_onfly(num_samples=100, graph_size=20):\n",
        "#     \"\"\"Generate temp dataset in memory\n",
        "#     \"\"\"\n",
        "\n",
        "#     depo = torch.FloatTensor(num_samples, 2).uniform_(0, 100) / 1200.\n",
        "#     graphs = torch.FloatTensor(num_samples, graph_size, 2).uniform_(0, 100) / 1200.\n",
        "#     demand = (torch.FloatTensor(num_samples, graph_size).uniform_(0, 50).int() + 1).float() / 200\n",
        "#     time = (torch.FloatTensor(num_samples, graph_size).uniform_(150, 1000).int() + 1).float() / 1200.\n",
        "#     finish = time + (torch.FloatTensor(num_samples, graph_size).uniform_(0, 200).int()).float() / 1200.\n",
        "#     service = (torch.FloatTensor(num_samples, graph_size).uniform_(0, 100).int() + 1).float() / 1200.\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "#     return (depo, graphs, demand, time, finish, service)\n",
        "\n",
        "\n",
        "\n",
        "class FastTensorDataLoader:\n",
        "    \"\"\"\n",
        "    A DataLoader-like object for a set of tensors that can be much faster than\n",
        "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
        "    the dataset and calls cat (slow).\n",
        "    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n",
        "    \"\"\"\n",
        "    def __init__(self, *tensors, batch_size=32, shuffle=False):\n",
        "        \"\"\"\n",
        "        Initialize a FastTensorDataLoader.\n",
        "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
        "        :param batch_size: batch size to load.\n",
        "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
        "            iterator is created out of this object.\n",
        "        :returns: A FastTensorDataLoader.\n",
        "        \"\"\"\n",
        "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
        "        self.tensors = tensors\n",
        "\n",
        "        self.dataset_len = self.tensors[0].shape[0]\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Calculate # batches\n",
        "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
        "        if remainder > 0:\n",
        "            n_batches += 1\n",
        "        self.n_batches = n_batches\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            r = torch.randperm(self.dataset_len)\n",
        "            self.tensors = [t[r] for t in self.tensors]\n",
        "        self.i = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.i >= self.dataset_len:\n",
        "            raise StopIteration\n",
        "        batch = tuple(t[self.i:self.i+self.batch_size] for t in self.tensors)\n",
        "        self.i += self.batch_size\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches\n",
        "\n",
        "\n",
        "def get_cur_time():\n",
        "    \"\"\"Returns local time as string\n",
        "    \"\"\"\n",
        "    ts = time.time()\n",
        "    return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_dev_of_mod(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = Data_100()\n",
        "data = generate_data_onfly(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0100, 0.0950, 0.1550,  ..., 0.0550, 0.1000, 0.0850],\n",
              "        [0.0450, 0.1750, 0.0050,  ..., 0.0850, 0.1050, 0.1450],\n",
              "        [0.0500, 0.0850, 0.1500,  ..., 0.0400, 0.0300, 0.1000],\n",
              "        ...,\n",
              "        [0.0800, 0.0300, 0.0700,  ..., 0.0550, 0.0850, 0.1000],\n",
              "        [0.0400, 0.0800, 0.1150,  ..., 0.1550, 0.1800, 0.0800],\n",
              "        [0.1150, 0.0100, 0.0350,  ..., 0.0750, 0.1100, 0.1350]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHk9tMop13Xq"
      },
      "source": [
        "Reinforce_Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zWeG7ayx15Q6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from scipy.stats import ttest_rel\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def copy_of_pt_model(model, embedding_dim=128, graph_size=20):\n",
        "\n",
        "\n",
        "    new_model = AttentionDynamicModel(embedding_dim).to(get_dev_of_mod(model))\n",
        "    set_decode_type(new_model, \"sampling\")\n",
        "\n",
        "    model_dict = model.state_dict()\n",
        "    new_model.load_state_dict(model_dict)\n",
        "\n",
        "    new_model.eval()\n",
        "\n",
        "    return new_model\n",
        "\n",
        "def get_costs_rollout(model, train_batches, disable_tqdm):\n",
        "    costs_list = []\n",
        "    for batch in tqdm(train_batches, disable=disable_tqdm, desc=\"Rollout greedy execution\"):\n",
        "        cost, _ = model(batch)\n",
        "        costs_list.append(cost)\n",
        "    return costs_list\n",
        "\n",
        "def rollout(model, dataset, batch_size = 32, disable_tqdm = False):\n",
        "    # Evaluate model in greedy mode\n",
        "    set_decode_type(model, \"greedy\")\n",
        "\n",
        "    train_batches = FastTensorDataLoader(dataset[0],dataset[1],dataset[2], dataset[3],dataset[4],dataset[5], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model_was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        costs_list = get_costs_rollout(model, train_batches, disable_tqdm)\n",
        "\n",
        "    if model_was_training: model.train() # restore original model training state\n",
        "\n",
        "    return torch.cat(costs_list, dim=0)\n",
        "\n",
        "\n",
        "def validate(dataset, model, batch_size=1000):\n",
        "    \"\"\"Validates model on given dataset in greedy mode\n",
        "    \"\"\"\n",
        "    # rollout will set the model to eval mode and turn it back to it's original mode after it finishes\n",
        "    val_costs = rollout(model, dataset, batch_size=batch_size)\n",
        "    set_decode_type(model, \"sampling\")\n",
        "    mean_cost = torch.mean(val_costs)\n",
        "    print(f\"Validation score: {np.round(mean_cost, 4)}\")\n",
        "    return mean_cost\n",
        "\n",
        "\n",
        "class RolloutBaseline:\n",
        "\n",
        "    def __init__(self, model, filename = None,\n",
        "                 from_checkpoint=False,\n",
        "                 path_to_checkpoint=None,\n",
        "                 cfg = Data_100(),\n",
        "                 wp_n_epochs=1,\n",
        "                 epoch=0,\n",
        "                 num_samples=10000,\n",
        "                 warmup_exp_beta=0.8,\n",
        "                 embedding_dim=128,\n",
        "                 graph_size=100\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: current model\n",
        "            filename: suffix for baseline checkpoint filename\n",
        "            from_checkpoint: start from checkpoint flag\n",
        "            path_to_checkpoint: path to baseline model weights\n",
        "            wp_n_epochs: number of warm-up epochs\n",
        "            epoch: current epoch number\n",
        "            num_samples: number of samples to be generated for baseline dataset\n",
        "            warmup_exp_beta: warmup mixing parameter (exp. moving average parameter)\n",
        "\n",
        "        \"\"\"\n",
        "        self.cfg = cfg\n",
        "        self.num_samples = num_samples\n",
        "        self.cur_epoch = epoch\n",
        "        self.wp_n_epochs = wp_n_epochs\n",
        "        self.beta = warmup_exp_beta\n",
        "\n",
        "        # controls the amount of warmup\n",
        "        self.alpha = 0.0\n",
        "\n",
        "        self.running_average_cost = None\n",
        "\n",
        "        # Checkpoint params\n",
        "        self.filename = filename\n",
        "        self.from_checkpoint = from_checkpoint\n",
        "        self.path_to_checkpoint = path_to_checkpoint\n",
        "\n",
        "        # Problem params\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.graph_size = graph_size\n",
        "\n",
        "        # create and evaluate initial baseline\n",
        "        self._update_baseline(model, epoch)\n",
        "\n",
        "\n",
        "    def _update_baseline(self, model, epoch):\n",
        "        self.model = copy_of_pt_model(model,\n",
        "                                          embedding_dim=self.embedding_dim,\n",
        "                                          graph_size=self.graph_size)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "        # We generate a new dataset for baseline model on each baseline update to prevent possible overfitting\n",
        "        self.dataset = generate_data_onfly(self.cfg)\n",
        "\n",
        "        print(f\"Evaluating baseline model on baseline dataset (epoch = {epoch})\")\n",
        "        self.bl_vals = rollout(self.model, self.dataset)\n",
        "        self.mean = torch.mean(self.bl_vals)\n",
        "        self.cur_epoch = epoch\n",
        "\n",
        "    def ema_eval(self, cost):\n",
        "        \"\"\"This is running average of cost through previous batches (only for warm-up epochs)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.running_average_cost is None:\n",
        "            self.running_average_cost = torch.mean(cost)\n",
        "        else:\n",
        "            self.running_average_cost = self.beta * self.running_average_cost + (1. - self.beta) * torch.mean(cost)\n",
        "\n",
        "        return self.running_average_cost\n",
        "\n",
        "    def eval(self, batch, cost):\n",
        "        \"\"\"Evaluates current baseline model on single training batch\n",
        "        \"\"\"\n",
        "\n",
        "        if self.alpha == 0:\n",
        "            return self.ema_eval(cost)\n",
        "\n",
        "        if self.alpha < 1:\n",
        "            v_ema = self.ema_eval(cost)\n",
        "        else:\n",
        "            v_ema = torch.tensor(0.0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            v_b, _ = self.model(batch)\n",
        "\n",
        "        # Combination of baseline cost and exp. moving average cost\n",
        "        return self.alpha * v_b.detach() + (1 - self.alpha) * v_ema.detach()\n",
        "\n",
        "    def eval_all(self, dataset, batch):\n",
        "        \"\"\"Evaluates current baseline model on the whole dataset only for non warm-up epochs\n",
        "        \"\"\"\n",
        "\n",
        "        if self.alpha < 1:\n",
        "            return None\n",
        "\n",
        "        val_costs = rollout(self.model, dataset, batch_size=batch)\n",
        "\n",
        "        return val_costs\n",
        "\n",
        "    def epoch_callback(self, model, epoch):\n",
        "        \"\"\"Compares current baseline model with the training model and updates baseline if it is improved\n",
        "        \"\"\"\n",
        "\n",
        "        self.cur_epoch = epoch\n",
        "\n",
        "        print(f\"Evaluating candidate model on baseline dataset (callback epoch = {self.cur_epoch})\")\n",
        "        candidate_vals = rollout(model, self.dataset)  # costs for training model on baseline dataset\n",
        "        candidate_mean = torch.mean(candidate_vals)\n",
        "\n",
        "        diff = candidate_mean - self.mean\n",
        "\n",
        "        print(f\"Epoch {self.cur_epoch} candidate mean {candidate_mean}, baseline epoch {self.cur_epoch} mean {self.mean}, difference {diff}\")\n",
        "\n",
        "        if diff < 0:\n",
        "            # statistic + p-value\n",
        "            t, p = ttest_rel(candidate_vals, self.bl_vals)\n",
        "\n",
        "            p_val = p / 2\n",
        "            print(f\"p-value: {p_val}\")\n",
        "\n",
        "            if p_val < 0.05:\n",
        "                print('Update baseline')\n",
        "                self._update_baseline(model, self.cur_epoch)\n",
        "\n",
        "        # alpha controls the amount of warmup\n",
        "        if self.alpha < 1.0:\n",
        "            self.alpha = (self.cur_epoch + 1) / float(self.wp_n_epochs)\n",
        "            print(f\"alpha was updated to {self.alpha}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QccWtUMQ2KYl"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jn68NI7m2LGS"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "class IterativeMean():\n",
        "    def __init__(self):\n",
        "        self.sum = 0\n",
        "        self.n = 0\n",
        "\n",
        "    def update_state(self, val):\n",
        "        self.sum += val\n",
        "        self.n += 1\n",
        "\n",
        "    def result(self):\n",
        "        return self.sum / self.n\n",
        "\n",
        "def train_model(optimizer,\n",
        "                model_torch,\n",
        "                baseline,\n",
        "                batch = 128,\n",
        "                start_epoch = 0,\n",
        "                end_epoch = 5,\n",
        "                from_checkpoint = False,\n",
        "                grad_norm_clipping = 1.0,\n",
        "                batch_verbose = 1000,\n",
        "                mem_efficient=True,\n",
        "                ):\n",
        "\n",
        "\n",
        "    def rein_loss(model, inputs, baseline, num_batch):\n",
        "        \"\"\"Calculate loss for REINFORCE algorithm\n",
        "        \"\"\"\n",
        "\n",
        "        # Evaluate model, get costs and log probabilities\n",
        "        cost, log_likelihood = model(inputs)\n",
        "\n",
        "        # Evaluate baseline\n",
        "        # For first wp_n_epochs we take the combination of baseline and ema for previous batches\n",
        "        # after that we take a slice of precomputed baseline values\n",
        "        bl_val = bl_vals[num_batch] if bl_vals is not None else baseline.eval(inputs, cost)\n",
        "\n",
        "        # Calculate loss\n",
        "        reinforce_loss = torch.mean((cost - bl_val.detach()) * log_likelihood)\n",
        "\n",
        "        return reinforce_loss, torch.mean(cost)\n",
        "\n",
        "    def mem_efficient_rein_loss(model, inputs, baseline, num_batch):\n",
        "        rein_detached_loss, (cost, log_likelihood) = model.fwd_rein_loss(inputs, baseline, bl_vals, num_batch)\n",
        "        return rein_detached_loss, torch.mean(cost)\n",
        "\n",
        "    def grad(model, inputs, baseline, num_batch):\n",
        "        \"\"\"Calculate gradients\n",
        "        \"\"\"\n",
        "        if mem_efficient:\n",
        "            loss, cost = mem_efficient_rein_loss(model, inputs, baseline, num_batch)\n",
        "        else:\n",
        "            loss, cost = rein_loss(model, inputs, baseline, num_batch)\n",
        "            loss.backward()\n",
        "        grads = [param.grad.view(-1) for param in model.parameters()]\n",
        "        grads = torch.cat(grads)\n",
        "        # we can return detached loss since it's backwarded already above\n",
        "        return loss.detach(), cost, grads\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    cfg = Data_100()\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "\n",
        "        # Create dataset on current epoch\n",
        "        data = generate_data_onfly(cfg)\n",
        "\n",
        "        epoch_loss_avg = IterativeMean()\n",
        "        epoch_cost_avg = IterativeMean()\n",
        "\n",
        "        # Skip warm-up stage when we continue training from checkpoint\n",
        "        if from_checkpoint and baseline.alpha != 1.0:\n",
        "            print('Skipping warm-up mode')\n",
        "            baseline.alpha = 1.0\n",
        "\n",
        "        # If epoch > wp_n_epochs then precompute baseline values for the whole dataset else None\n",
        "        bl_vals = baseline.eval_all(data, batch)  # (samples, ) or None\n",
        "        bl_vals = torch.reshape(bl_vals, (-1, batch)) if bl_vals is not None else None # (n_batches, batch) or None\n",
        "\n",
        "\n",
        "\n",
        "        train_batches = FastTensorDataLoader(data[0],data[1],data[2], data[3],data[4],data[5], batch_size=batch, shuffle=False)\n",
        "\n",
        "        for num_batch, x_batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            loss_value, cost_val, grads = grad(model_torch, x_batch, baseline, num_batch)\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model_torch.parameters(), grad_norm_clipping)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track progress\n",
        "            epoch_loss_avg.update_state(loss_value)\n",
        "            epoch_cost_avg.update_state(cost_val)\n",
        "\n",
        "            if num_batch%batch_verbose == 0:\n",
        "                print(\"Epoch {} (batch = {}): Loss: {}: Cost: {}\".format(epoch, num_batch, epoch_loss_avg.result(), epoch_cost_avg.result()))\n",
        "\n",
        "        # Update baseline if the candidate model is good enough. In this case also create new baseline dataset\n",
        "        baseline.epoch_callback(model_torch, epoch)\n",
        "        set_decode_type(model_torch, \"sampling\")\n",
        "        print('--------------')\n",
        "\n",
        "\n",
        "\n",
        "        print(get_cur_time(), \"Epoch {}: Loss: {}: Cost: {}\".format(epoch, epoch_loss_avg.result(), epoch_cost_avg.result()))\n",
        "        print('--------------')\n",
        "        torch.save(model.state_dict(), 'twdynamic_cvrp_100.pt')\n",
        "        print('Saved!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eZX8lVoc9aO5"
      },
      "outputs": [],
      "source": [
        "BATCH = 32\n",
        "START_EPOCH = 0\n",
        "END_EPOCH = 200\n",
        "FROM_CHECKPOINT = False\n",
        "embedding_dim = 128\n",
        "LEARNING_RATE = 0.001\n",
        "ROLLOUT_SAMPLES = 1000\n",
        "NUMBER_OF_WP_EPOCHS = 1\n",
        "GRAD_NORM_CLIPPING = 1.0\n",
        "BATCH_VERBOSE = 5\n",
        "SEED = 1234\n",
        "GRAPH_SIZE = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LycNP-C49fdy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model = AttentionDynamicModel(embedding_dim).to(device)\n",
        "# set_decode_type(model, \"sampling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG4KQJAfHM46",
        "outputId": "b8e76d23-3201-47f9-fe42-9dd08f2dd3b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([5.4446, 5.7982]), tensor([-379.3987, -371.0754], grad_fn=<AddBackward0>), tensor([[ 72,  51,   8,  85,  48,  42,  25,  47,  19,  13,  34,  66,   0,  29,\n",
            "         100,  95,  78,  23,  96,   6,  91,  84,  89,  10,  76,   0,  35,   0,\n",
            "          99,  11,  93,  80,  38,  32,  59,   3,  87,  64,  14,  45,  62,   0,\n",
            "          37,   0,  16,  46,  71,  26,  53,  69,  56,  49,  90,  82,  61,  12,\n",
            "           0,  94,  15,   0,  60,  17,  65,  79,  43,   2,  73,  31,  77,  67,\n",
            "          27,  88,  40,  24,  28,  41,   0,   7,  33,   9,  50,  58,  18,  36,\n",
            "          83,  54,  92,  86,  21,  70,   0,  55,  20,   4,   0,  74,   0,  22,\n",
            "          68,   0,  30,  97,  57,   5,  63,  81,  44,  75,  98,   1,   0,  39,\n",
            "           0,  52],\n",
            "        [ 38,  43,  59,   0,  79,  69,  10,  97,  81,  70, 100,  14,  39,  49,\n",
            "          41,  66,   2,   0,  30,  98,  71,  54,   7,  83,  63,  86,  26,  15,\n",
            "          75,   8,  78,  73,  47,  19,   0,  57,  17,  80,  65,  45,  18,  27,\n",
            "           3,  60,  37,  87,  32,  61,  74,   4,   6,  46,   0,  92,  84,  51,\n",
            "          55,   0,  23,  53,   0,  11,   5,  48,  68,  28,  20,  40,  13,  56,\n",
            "          67,  95,   0,  82,  62,  99,  93,  76,   1,  90,  42,  33,  21,  88,\n",
            "          50,   0,  58,  12,  31,  77,  36,  35,  89,  16,  72,  24,  85,  34,\n",
            "          64,   0,  29,  44,  94,   9,  22,  52,  25,   0,  91,   0,  96,   0,\n",
            "           0,   0]])]\n"
          ]
        }
      ],
      "source": [
        "cfg = Data_100()\n",
        "data = generate_data_onfly(cfg)\n",
        "model = AttentionDynamicModel(embedding_dim).to(device)\n",
        "set_decode_type(model, \"sampling\")\n",
        "train_batches = FastTensorDataLoader(data[0],data[1],data[2], data[3],data[4],data[5], batch_size=2, shuffle=False)\n",
        "for batch in train_batches:\n",
        "  # print(batch)\n",
        "  a = model(batch, return_pi = True)\n",
        "  print(a)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mn9xNQ5q9tWC"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3Ajttmv9ySl",
        "outputId": "feb71a77-e555-4eed-ae2c-2dfe7d33ff72"
      },
      "outputs": [],
      "source": [
        "# # Initialize baseline\n",
        "# baseline = RolloutBaseline(model,\n",
        "#                            wp_n_epochs = NUMBER_OF_WP_EPOCHS,\n",
        "#                            epoch = 0,\n",
        "#                            num_samples=ROLLOUT_SAMPLES,\n",
        "#                            filename = None,\n",
        "#                            from_checkpoint = FROM_CHECKPOINT,\n",
        "#                            embedding_dim=embedding_dim,\n",
        "#                            graph_size=GRAPH_SIZE\n",
        "#                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1hGUijcu-2u6"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h8k4GFZ-3Dr",
        "outputId": "a240fc7f-da97-40e0-c3ca-2cebbccfc130"
      },
      "outputs": [],
      "source": [
        "# train_model(optimizer,\n",
        "#             model,\n",
        "#             baseline,\n",
        "#             batch = BATCH,\n",
        "#             start_epoch = START_EPOCH,\n",
        "#             end_epoch = END_EPOCH,\n",
        "#             from_checkpoint = FROM_CHECKPOINT,\n",
        "#             grad_norm_clipping = GRAD_NORM_CLIPPING,\n",
        "#             batch_verbose = BATCH_VERBOSE,\n",
        "#             )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = Data_100()\n",
        "data = generate_data_onfly(cfg) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DNNJF00TaB81"
      },
      "outputs": [],
      "source": [
        "from Load_data import load_data\n",
        "file = ['R101.txt', 'R103.txt', 'R102.txt', 'R112.txt', 'R106.txt', 'R107.txt', 'R105.txt', 'R111.txt', 'C108.txt', \n",
        " 'RC208.txt', 'C109.txt', 'R110.txt', 'R104.txt', 'R201.txt', 'R202.txt', 'R203.txt', 'R207.txt', 'R206.txt',\n",
        "'R210.txt', 'R204.txt', 'C208.txt', 'R205.txt', 'R211.txt', 'RC108.txt', 'RC105.txt', 'R208.txt', 'C205.txt',\n",
        "'C204.txt', 'R209.txt', 'RC104.txt', 'RC106.txt', 'C206.txt', 'C207.txt', 'RC107.txt', 'RC103.txt', 'C203.txt', \n",
        "'C202.txt', 'RC102.txt', 'C201.txt', 'RC101.txt', 'R109.txt', 'C104.txt', 'RC204.txt', 'RC205.txt', 'C105.txt',\n",
        "'R108.txt', 'C107.txt', 'RC207.txt', 'RC206.txt', 'C106.txt', 'C102.txt', 'RC202.txt', 'RC203.txt', 'C103.txt',\n",
        "'C101.txt', 'RC201.txt']\n",
        "\n",
        "idx = 0\n",
        "with open('Data.txt', 'w') as outfile:\n",
        "    file_path = 'txt/' + file[idx]\n",
        "    with open(file_path) as infile:\n",
        "        for line in infile:\n",
        "            outfile.write(line)\n",
        "\n",
        "max_cap, xcoord, ycoord, demand, e_time, l_time, s_time, _ = load_data() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#     depo = torch.FloatTensor(num_samples, 2).uniform_(0, 100) / 1200.\n",
        "#     graphs = torch.FloatTensor(num_samples, graph_size, 2).uniform_(0, 100) / 1200.\n",
        "#     demand = (torch.FloatTensor(num_samples, graph_size).uniform_(0, 50).int() + 1).float() / 200\n",
        "#     time = (torch.FloatTensor(num_samples, graph_size).uniform_(150, 1000).int() + 1).float() / 1200.\n",
        "#     finish = time + (torch.FloatTensor(num_samples, graph_size).uniform_(0, 200).int()).float() / 1200.\n",
        "#     service = (torch.FloatTensor(num_samples, graph_size).uniform_(0, 100).int() + 1).float() / 1200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = ['R101.txt', 'R103.txt', 'R102.txt', 'R112.txt', 'R106.txt', 'R107.txt', 'R105.txt', 'R111.txt', 'C108.txt', \n",
        " 'RC208.txt', 'C109.txt', 'R110.txt', 'R104.txt', 'R201.txt', 'R202.txt', 'R203.txt', 'R207.txt', 'R206.txt',\n",
        "'R210.txt', 'R204.txt', 'C208.txt', 'R205.txt', 'R211.txt', 'RC108.txt', 'RC105.txt', 'R208.txt', 'C205.txt',\n",
        "'C204.txt', 'R209.txt', 'RC104.txt', 'RC106.txt', 'C206.txt', 'C207.txt', 'RC107.txt', 'RC103.txt', 'C203.txt', \n",
        "'C202.txt', 'RC102.txt', 'C201.txt', 'RC101.txt', 'R109.txt', 'C104.txt', 'RC204.txt', 'RC205.txt', 'C105.txt',\n",
        "'R108.txt', 'C107.txt', 'RC207.txt', 'RC206.txt', 'C106.txt', 'C102.txt', 'RC202.txt', 'RC203.txt', 'C103.txt',\n",
        "'C101.txt', 'RC201.txt'] \n",
        "model.load_state_dict(torch.load('D-AM.pt', map_location=torch.device('cpu')))\n",
        "for idx in range(len(file)):\n",
        "    with open('Data.txt', 'w') as outfile:\n",
        "        file_path = 'txt/' + file[idx]\n",
        "        with open(file_path) as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)\n",
        "\n",
        "    max_cap, xcoord, ycoord, demand, e_time, l_time, s_time, _ = load_data() \n",
        "\n",
        "    scale1 = l_time[0]\n",
        "    scale2 = max_cap \n",
        "    num_samples = 10 \n",
        "    depo = torch.FloatTensor([[xcoord[0], ycoord[0]]]).repeat(num_samples, 1) / scale1\n",
        "    graphs = torch.FloatTensor(np.stack((xcoord[1:], ycoord[1:]), axis=1)).repeat(num_samples, 1, 1) / scale1\n",
        "    demand = torch.FloatTensor(demand[1:]).repeat(num_samples, 1) / scale2 \n",
        "    time = torch.FloatTensor(e_time[1:]).repeat(num_samples, 1) / scale1\n",
        "    finish = torch.FloatTensor(l_time[1:]).repeat(num_samples, 1) / scale1\n",
        "    service = torch.FloatTensor(s_time[1:]).repeat(num_samples, 1) / scale1\n",
        "    assert depo.shape == (num_samples, 2)\n",
        "    assert graphs.shape == (num_samples, 100, 2)\n",
        "    assert demand.shape == (num_samples, 100)\n",
        "    assert time.shape == (num_samples, 100)\n",
        "    assert finish.shape == (num_samples, 100)\n",
        "    assert service.shape == (num_samples, 100)\n",
        "\n",
        "    data = (depo, graphs, demand, time, finish, service)\n",
        "    set_decode_type(model, \"sampling\")\n",
        "    train_batches = FastTensorDataLoader(data[0],data[1],data[2], data[3],data[4],data[5], batch_size=5, shuffle=False)\n",
        "    lst = []\n",
        "    for batch in train_batches:\n",
        "        a = model(batch, return_pi = True)\n",
        "        lst.append(torch.min(a[0]*scale1).item())\n",
        "    with open('Data.txt', 'r') as f:\n",
        "        name = f.readline()\n",
        "    name.rstrip()\n",
        "    with open('AM.txt', 'a') as f:\n",
        "        f.write(name + '\\n')\n",
        "        f.write(\"Distance: {}\".format(lst[0])+ '\\n')\n",
        "        f.write(\"------------------------------\"+ '\\n')\n",
        "    with open('D-AM.txt', 'a') as f:\n",
        "        f.write(name + '\\n')\n",
        "        f.write(\"Distance: {}\".format(lst[1])+ '\\n')\n",
        "        f.write(\"------------------------------\"+ '\\n')\n",
        "\n",
        "with open('AM.txt', 'a') as f:\n",
        "        f.write(name + '\\n')\n",
        "        f.write(\"Distance: {}\".format(lst[0])+ '\\n')\n",
        "        f.write(\"----------------------------------------------------------------------------------------------\"+ '\\n')\n",
        "with open('D-AM.txt', 'a') as f:\n",
        "        f.write(name + '\\n')\n",
        "        f.write(\"Distance: {}\".format(lst[1])+ '\\n')\n",
        "        f.write(\"----------------------------------------------------------------------------------------------\"+ '\\n')\n",
        "\n",
        "file = ['data_gen/gen0.txt', 'data_gen/gen1.txt','data_gen/gen2.txt','data_gen/gen3.txt',\n",
        "        'data_gen/gen4.txt','data_gen/gen5.txt','data_gen/gen6.txt','data_gen/gen7.txt',\n",
        "        'data_gen/gen8.txt','data_gen/gen9.txt']\n",
        "\n",
        "for idx in range(len(file)):\n",
        "    with open('Data.txt', 'w') as outfile:\n",
        "        file_path = file[idx]\n",
        "        with open(file_path) as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)\n",
        "\n",
        "    max_cap, xcoord, ycoord, demand, e_time, l_time, s_time, _ = load_data() \n",
        "\n",
        "    scale1 = l_time[0]\n",
        "    scale2 = max_cap \n",
        "    num_samples = 10 \n",
        "    depo = torch.FloatTensor([[xcoord[0], ycoord[0]]]).repeat(num_samples, 1) / scale1\n",
        "    graphs = torch.FloatTensor(np.stack((xcoord[1:], ycoord[1:]), axis=1)).repeat(num_samples, 1, 1) / scale1\n",
        "    demand = torch.FloatTensor(demand[1:]).repeat(num_samples, 1) / scale2 \n",
        "    time = torch.FloatTensor(e_time[1:]).repeat(num_samples, 1) / scale1\n",
        "    finish = torch.FloatTensor(l_time[1:]).repeat(num_samples, 1) / scale1\n",
        "    service = torch.FloatTensor(s_time[1:]).repeat(num_samples, 1) / scale1\n",
        "    assert depo.shape == (num_samples, 2)\n",
        "    assert graphs.shape == (num_samples, 100, 2)\n",
        "    assert demand.shape == (num_samples, 100)\n",
        "    assert time.shape == (num_samples, 100)\n",
        "    assert finish.shape == (num_samples, 100)\n",
        "    assert service.shape == (num_samples, 100)\n",
        "\n",
        "    data = (depo, graphs, demand, time, finish, service)\n",
        "    set_decode_type(model, \"sampling\")\n",
        "    train_batches = FastTensorDataLoader(data[0],data[1],data[2], data[3],data[4],data[5], batch_size=5, shuffle=False)\n",
        "    lst = []\n",
        "    for batch in train_batches:\n",
        "        a = model(batch, return_pi = True)\n",
        "        lst.append(torch.min(a[0]*scale1).item())\n",
        "    with open('Data.txt', 'r') as f:\n",
        "        name = f.readline()\n",
        "    name.rstrip()\n",
        "    with open('AM.txt', 'a') as f:\n",
        "        f.write(name + '\\n')\n",
        "        f.write(\"Distance: {}\".format(lst[0])+ '\\n')\n",
        "        f.write(\"---------------------------\"+ '\\n')\n",
        "    with open('D-AM.txt', 'a') as f:\n",
        "        f.write(name + '\\n')\n",
        "        f.write(\"Distance: {}\".format(lst[1])+ '\\n')\n",
        "        f.write(\"---------------------------\"+ '\\n')\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
